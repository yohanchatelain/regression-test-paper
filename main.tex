\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

% language
\usepackage[english]{babel}
\usepackage{caption}
% \usepackage{subcaption}
\usepackage{makecell}
% \usepackage[toc,page]{appendix}
\usepackage{todonotes}
\usepackage{soul}
\usepackage{pdflscape}
% Set page size and margins Replace `letterpaper' with `a4paper' for UK/EU
% standard size

% Useful packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{xcolor}% http://ctan.org/pkg/xcolor
\usepackage{hyperref}
\usepackage{colortbl}
\usepackage{dsfont}

\usepackage{todonotes}
\usepackage{etoolbox}
\usepackage{xargs}
\usepackage{multirow}

\usepackage[switch]{lineno}

% Fix link colors
\hypersetup{
    colorlinks = true,
    linkcolor=red,
    citecolor=red,
    urlcolor=blue,
    linktocpage % so that page numbers are clickable in toc
}

\newcommand{\TODO}[1]{\color{red}\textsc{TODO:} #1\color{black}\xspace}
\newcommand{\TG}[1]{\color{orange}\textsc{TG:} #1\color{black}\xspace}
\newcommand{\oesteban}[1]{\color{orange}\textsc{OE:} #1\color{black}\xspace}
\newcommand{\fmriprep}{\emph{fMRIPrep}\xspace}
\newcommand{\fwhm}{\textsc{FWHM}}

\newcommand{\failcolor}{orange} % HEX #009E73
\newcommand{\passcolor}{green} % HEX #D55E00

\hyphenation{pre-processing}


\newcommand\Mark[2][8.4]{%
  \rlap{\tikz[baseline=(current bounding box.south)]{
        \shade[left color=\color{red}, right color=\color{green}, middle color=\color{yellow}]
               (0,0) rectangle ++(#1*#2/100,0.3);}%
  }%
}

\newtoggle{RRandRS}
\togglefalse{RRandRS} % Set it to true or false based on your requirement


\newlength{\figureWidthUM}
\newlength{\vspaceIndex}
\newlength{\rowSpace}

\newcommandx{\uncertaintyMap}[5][5=false]{%
  % arguments:
  % 1: FWHM 
  % 2: subject index
  % 3: dataset
  % 4: subject
  % 5: add columns title
%   \iftoggle{RRandRS}{\setlength{\figureWidthUM}{0.2\paperheight}}{\setlength{\figureWidthUM}{0.27\paperheight}}
  \setlength{\figureWidthUM}{0.15\paperheight}
  \ifstrequal{#5}{true}{\setlength{\vspaceIndex}{-45pt}}{\setlength{\vspaceIndex}{25pt}}
  \begin{tabular}[b]{@{}c@{}}
    #2\vspace*{\vspaceIndex}
  \end{tabular}
  \begin{tabular}[t]{@{}c@{}}
    \ifstrequal{#5}{true}{IEEE (T1 intensity)}{} \\
    \includegraphics[width=\figureWidthUM]{figures/sig/#1mm/ieee_#3_#4.pdf}
  \end{tabular}
  \begin{tabular}[t]{@{}c@{}}
    \ifstrequal{#5}{true}{RR (significant bits)}{} \\
    \includegraphics[width=\figureWidthUM]{figures/sig/#1mm/rr_#3_#4_sig.pdf}
  \end{tabular}
  \begin{tabular}[t]{@{}c@{}}
    \ifstrequal{#5}{true}{RS (significant bits)}{} \\
    \includegraphics[width=\figureWidthUM]{figures/sig/#1mm/rs_#3_#4_sig.pdf}
  \end{tabular}
  \iftoggle{RRandRS}{%
    \begin{tabular}[t]{@{}c@{}}
      \ifstrequal{#5}{true}{RR+RS (significant bits)}{} \\
      \includegraphics[width=\figureWidthUM]{figures/sig/#1mm/rr.rs_#3_#4_sig.pdf}
    \end{tabular}
  }{}
}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

\newcommandx{\uncertaintyMapDiscrete}[5][5=false]{%
  % arguments:
  % 1: FWHM 
  % 2: subject index
  % 3: dataset
  % 4: subject
  % 5: add columns title
%   \iftoggle{RRandRS}{\setlength{\figureWidthUM}{0.2\paperheight}}{\setlength{\figureWidthUM}{0.27\paperheight}}
  \setlength{\figureWidthUM}{0.16\paperheight}
  \ifstrequal{#5}{true}{\setlength{\vspaceIndex}{-45pt}}{\setlength{\vspaceIndex}{15pt}}
  % \begin{tabular}[b]{@{}c@{}}
  %   #2\vspace*{\vspaceIndex}
  % \end{tabular}
  \ifstrequal{#5}{true}{
    \begin{tabular}[t]{@{}P{\figureWidthUM}@{}P{\figureWidthUM}@{}P{\figureWidthUM}@{}}
      IEEE (T1 intensity) & RR (significant bits) & RS (significant bits) 
    \end{tabular}
  }{}
  \begin{tabular}[t]{m{3pt}P{\figureWidthUM}@{}P{\figureWidthUM}@{}P{\figureWidthUM}@{}}
    {#2 \vspace*{0.25\figureWidthUM}} & \includegraphics[width=\figureWidthUM]{figures/sig/#1mm/ieee_#3_#4.pdf} &
    \includegraphics[width=\figureWidthUM]{figures/sig/discrete/#1mm/rr_#3_#4_sig_discrete.pdf} &
    \includegraphics[width=\figureWidthUM]{figures/sig/discrete/#1mm/rs_#3_#4_sig_discrete.pdf}
  \end{tabular}
  \iftoggle{RRandRS}{%
    \begin{tabular}[t]{@{}c@{}}
      \ifstrequal{#5}{true}{RR+RS (significant bits)}{} \\
      \includegraphics[width=\figureWidthUM]{figures/sig/discrete/#1mm/rr.rs_#3_#4_sig_discrete.pdf}
    \end{tabular}
  }{}
}

\newcommandx{\uncertaintyMapDiscreteAll}[1]{%
  \centering
  \setlength{\rowSpace}{-24pt}
  \uncertaintyMapDiscrete{#1}{1}{ds000256}{sub-CTS201}[true] \\
  \vspace*{\rowSpace}
  \uncertaintyMapDiscrete{#1}{2}{ds000256}{sub-CTS210} \\
  \vspace*{\rowSpace}
  \uncertaintyMapDiscrete{#1}{3}{ds001600}{sub-1} \\
  \vspace*{\rowSpace}
  \uncertaintyMapDiscrete{#1}{4}{ds001748}{sub-adult15} \\
  \vspace*{\rowSpace}
  \uncertaintyMapDiscrete{#1}{5}{ds001748}{sub-adult16} \\
  \vspace*{\rowSpace}
  \uncertaintyMapDiscrete{#1}{6}{ds001771}{sub-36} \\
  \vspace*{\rowSpace}
  \uncertaintyMapDiscrete{#1}{7}{ds002338}{sub-xp201} \\
  \vspace*{\rowSpace}
  \uncertaintyMapDiscrete{#1}{8}{ds002338}{sub-xp207} \\
}

% Cluster failure: Why fMRI inferences for spatial extent have inflated
% false-positive https://www.pnas.org/doi/pdf/10.1073/pnas.1602413113

\title{A numerical variability approach to results stability tests and its application to neuroimaging}
\author{\IEEEauthorblockN{Yohan Chatelain\IEEEauthorrefmark{1}, Lo\"ic Tetrel\IEEEauthorrefmark{2}, Christopher J. Markiewicz\IEEEauthorrefmark{3}, Mathias Goncalves\IEEEauthorrefmark{3}, Gregory Kiar\IEEEauthorrefmark{6},\\ Oscar Esteban\IEEEauthorrefmark{3}\IEEEauthorrefmark{5},  Pierre Bellec\IEEEauthorrefmark{2}\IEEEauthorrefmark{4}, Tristan Glatard\IEEEauthorrefmark{1}\vspace*{0.2cm}}

\IEEEauthorblockA{\IEEEauthorrefmark{1}Department of Computer Science and Software Engineering\\ Concordia University, Montreal, Quebec, Canada.}

\IEEEauthorblockA{\IEEEauthorrefmark{2} Centre de recherche de l'Institut Universitaire de Gériatrie\\ de Montréal (CRIUGM), Montréal, Québec, Canada.}

\IEEEauthorblockA{\IEEEauthorrefmark{3} Department of Psychology, Stanford University, Stanford, CA, USA.}

\IEEEauthorblockA{\IEEEauthorrefmark{4} Department of Psychology, Université de Montréal, Montréal, Québec, Canada.}

\IEEEauthorblockA{\IEEEauthorrefmark{5} Department of Radiology, Lausanne University Hospital\\ and University of Lausanne, Switzerland.}

\IEEEauthorblockA{\IEEEauthorrefmark{6} Child Mind Institute, New York City, NY, USA.}


}

% Define the macros for added and removed text with line-break handling
\newcommand{\add}[1]{\textcolor{green!60!black}{#1\\}} % Add text in green
\newcommand{\remove}[1]{\leavevmode\noindent\textcolor{red}{#1\\}}         % Remove text in red


\begin{document}
\linenumbers
\maketitle

\begin{abstract}
  Ensuring the long-term reproducibility of data analyses requires results stability tests to verify that analysis results remain within acceptable variation bounds despite inevitable software updates and hardware evolutions. This paper introduces a numerical variability approach for results stability tests, which determines acceptable variation bounds using random rounding of floating-point calculations. By applying the resulting stability test to \fmriprep, a widely-used neuroimaging tool, we show that the test is sensitive enough to detect subtle updates in image processing methods while remaining specific enough to accept numerical variations within a reference version of the application. This result contributes to enhancing the reliability and reproducibility of data analyses by providing a robust and flexible method for stability testing.
\end{abstract}

\section{Introduction}

Data analyses can produce different results depending on the hardware and
software conditions in which they are executed~\cite{gronenschild2012effects},
which has important repercussions in several disciplines. This paper
investigates results stability tests whereby the outcome of data analysis is
asserted to remain within acceptable variation bounds of a reference result.
The primary challenge in developing such stability tests lies in the
determination of acceptable bounds of variation around the reference result. We
define such bounds from the numerical variability of the results, that is, from
the variability inherent to numerical computations.

We focus on the use case of neuroimaging analyses, although our method applies
to data analyses more broadly. For several decades, the neuroimaging community
has developed advanced software tools (e.g.,
\emph{FSL}~\cite{jenkinson2012fsl},
\emph{FreeSurfer}~\cite{fischl2012freesurfer},
\emph{ANTs}~\cite{avants2009advanced}, or \emph{AFNI}~\cite{COX1996162})
enabling researchers to study the human brain with unprecedented detail and
precision. Since neuroimaging tools now underpin scientific findings in several
disciplines, it is imperative to thoroughly test them. In particular,
neuroimaging studies often follow subjects over multiple years, which requires
data analyses to be consistent over substantial periods.

While existing research has focused on best practices for the accuracy and
consistency of neuroimaging results~\cite{tustison2013instrumentation}, it is
also important to consider the computational environments. Indeed, software
packages or operating systems can significantly impact specific area studies
such as brain surface reconstruction and cortical thickness quantification.
Notably, research has highlighted substantial differences across software
packages working on identical brain data~\cite{mikhael2019controlled}.
Moreover, studies underscored the effects of operating systems on measured
cortical thickness across various software packages and
versions~\cite{glatard2015reproducibility}. \add{Finally, Vila et
  al.~\cite{vila2024impact} demonstrated that hardware, software, and numerical
  variability can induce perturbations of similar magnitude, though these effects
  are uncorrelated.}

The present paper focuses primarily on the fMRIPrep
software~\cite{esteban2019fmriprep}, a tool to pre-process Magnetic Resonance
Imaging (MRI) data as a pre-requisite of any further analysis. fMRIPrep is an
integrated data analysis pipeline for structural and functional MRI
pre-processing. We focus on the pre-processing of structural MRI, which
includes intensity non-uniformity correction, skull stripping, and spatial
normalization to a brain template. In response to these computational
challenges that impact result stability, fMRIPrep developers recently initiated
long-term support (LTS) releases to guarantee results stability over multiple
years which is critical in longitudinal analysis. This motivated the
development of the stability tests presented in this paper.

Our stability tests leverage the numerical variability resulting that arises
from using finite-precision arithmetic in calculations. Such variability
primarily emerges due to changes in computational environments, which includes
hardware architecture, parallelization scheme, operating system, and software
dependencies. To estimate numerical variability, we rely on random
rounding~\cite{forsythe1959reprint}, a stochastic arithmetic technique that
randomly rounds floating-point operation results to the previous or to the next
floating-point number. Stochastic arithmetic has been successfully applied to
simulate numerical variability in various domains, including
neuroimaging~\cite{salari2021accurate,kiar2021numerical}. Our approach is not
specific to any particular numerical scheme and relies on a few statistical
assumptions such as normality and independence, making it applicable to a wide
range of scenarios.

In summary, the main contributions of this paper are the following:
\begin{itemize}
  \item Define a numerical variability approach to results stability tests;
  \item Build results stability tests for structural pre-processing in \fmriprep;
  \item Evaluate results of stability tests in several configurations.
\end{itemize}

\section{Results stability tests design}

Considering a data processing application $\Lambda$, the objective of our
stability test is to determine whether the results generated by a different
application $\tilde \Lambda$ significantly differ from the reference results
produced by $\Lambda$. In practice, we are particularly interested in the case
where $\tilde \Lambda$ corresponds to a different version of $\Lambda$ or the
same version executed in a different execution environment (operating system,
parallelization or hardware).

Given input data $I$, we assume that $\Lambda$ and $\tilde \Lambda$ produce
images $X$ and $\tilde X$ sampled on the same imaging grid (i.e., they showcase
the same number $v$ of voxels, and their orientation and resolution in physical
coordinates do not differ by more than a very small error $\epsilon$). We model
$X$ as a random variable and we sample its distribution by computing $n$ random
numerical perturbations of $\Lambda$, resulting in $n$ images $X_k$.
Conversely, we compute $\tilde X$ without random perturbation using the
IEEE-754 norm~\cite{ieee754}, to avoid computational overheads at test time.

The statistical test used for determining if '$\tilde X$ belongs to $X$' is
discussed in detail in Subsection~\ref{subsec:statistical_model}. To sample
reference distribution, we employ two methodologies, further elaborated in
Subsection \ref{subsec:numerical_variablity_model}. To capture anatomical
variability and other sources of variability arising from the scanning device
and sequence parameters, we test a diverse set of individuals collected from
several studies, as discussed in Subsection \ref{subsec:data}. To achieve
precise reproducibility of the unperturbed sample, we control variables such as
random seeding and multi-threading within the application, as detailed in
Subsection \ref{subsec:computing_infrastructure}. Table~\ref{tab:notations}
summarizes our notations and Figure~\ref{fig:test_workflow} provides a
comprehensive overview of our test workflow.

\subsection{Statistical model}
\label{subsec:statistical_model}

We preprocess the computed images $X,\tilde X$, leading to images $X^\perp,
  \tilde X^\perp$ (see~\ref{subsec:preprocessing}). To test if $\tilde X^\perp$
belongs to the distribution of $X^\perp$, we perform a $z$-test for each voxel
within the union of $B_k$ masks, $\tilde x_i$ ($i\leq v$), using the mean $\hat
  \mu_i$ and the standard deviation $\hat \sigma_i$ estimated from the $k$
perturbed output. The test computes a $p$-value $p_i$ under the null hypothesis
$H_{0,i}$ that the tested voxel belongs to the reference distribution:
\begin{equation} \label{eq:pval}
  p_i(z_i) = 2 \left(1-\Phi(z_i)\right),
\end{equation}
where $\Phi$ is the cumulative distribution function of the \add{standard} Gaussian, and
\begin{equation*}
  z_i = \frac{\tilde x_i-\hat \mu_i}{\hat \sigma_i},
\end{equation*}
where is the intensity of a voxel $\tilde x_i \in \tilde X_k^\perp$, $v$ is the number of voxels within the union mask,
and $\hat \mu_i$ and $\hat \sigma_i$ are the mean and standard deviation voxel intensities estimated
from the $k$ perturbed results $X_k^\perp$.
$H_{0,i}$ is rejected when $p_i$ is lower than a threshold $\alpha$ that also defines the confidence level of the test (1-$\alpha$)\%.
The $z$-test assumes that perturbed voxel intensities are normally distributed.

\begin{figure}
  \centering
  \includegraphics[width=.95\columnwidth]{figures/workflow_V.pdf}
  \caption{Stability test construction and evaluation. The reference application $\Lambda$ is executed $n$ times on the input data, with random perturbations, resulting in images $X_k$ from which a null distribution is built to test the result produced by target application $\tilde \Lambda$. All results undergo the same smoothing and normalization steps.}
  \label{fig:test_workflow}
\end{figure}

The test defined in Equation~\ref{eq:pval} consists of independent $z$-tests
performed for each of the $v$ voxels from each test image, resulting in a set
of $v$ $p$-values $p_i$, $i \leq v$. Because $v$ is in the order of one million
(depending on the image's resolution and brain size), it is critical to correct
for multiple comparisons and set an upper bound to false positive
tests~\cite{NICHOLS2007246}. We adjust the significance $\alpha$ level with
classical Bonferroni correction~\cite{farcomeni2008review}. Therefore, the
tested \fmriprep result is considered out of the reference distribution iif:
\begin{equation}
  \label{eq:bonferroni}
  \exists i \leq v \ \text{s.t.} \ p_i \leq \frac{\alpha}{v}.
\end{equation}

\begin{table}
  \centering
  \begin{tabular}{r|l}
    $\Lambda$          & reference application                                                \\
    $\tilde \Lambda$   & tested application                                                   \\
    $i$                & index of a voxel in B                                                \\
    $v$                & number of voxels in B                                                \\
    $k$                & index of a perturbed sample                                          \\
    $n$                & number of perturbed samples                                          \\
    $\tilde x_i$       & voxel intensity in image $\tilde X$                                  \\
    $z_i$              & z-score of voxel i                                                   \\
    $p_i$              & p-value of voxel i                                                   \\
    $\hat{\mu_i}$      & average voxel intensity across perturbed samples                     \\
    $\hat{\sigma_i}$   & standard deviation of voxel intensity across perturbed samples       \\
    $B_k$              & brain masks produced by random numerical perturbations of $\Lambda$  \\
    $B$                & union of $B_k$ masks                                                 \\
    $\tilde X$         & image produced by unperturbed $\tilde \Lambda$ using IEEE convention \\
    $X_k$              & images produced by random numerical perturbations of $\Lambda$       \\
    $X_k^{\perp}$      & $X_k$ masked with $B$, min-max scaled and spatially smoothed         \\
    $\hat{s}$          & average number of significant bits across the image                  \\
    $\mathcal{N}(0,1)$ & standard Gaussian distribution                                       \\
    $\Phi$             & cumulative distribution function of $\mathcal{N}(0,1)$               \\
    $\chi^2_{n-1}$     & Chi-2 distribution with n-1 degrees of freedom                       \\
  \end{tabular}
  \caption{Notations}
  \label{tab:notations}
\end{table}

\subsection{Numerical variability estimation}  % OE: merge with previous subsection?
\label{subsec:numerical_variablity_model}

To estimate the distribution of reference results and compute $\hat \mu_i$ and
$\hat \sigma_i$, we sample results distributions by applying two types of
random numerical perturbations: (1) Random Rounding (RR), which randomly rounds
function outputs in the \emph{GNU libmath} mathematical library, and (2) Random
Seed (RS), which varies the random seed used in the application. In
neuroimaging, random seeds are typically employed to initialize optimization
processes encountered in spatial normalization.

Random Rounding (RR) consists in rounding the exact result of a floating-point
arithmetic operation toward the previous or next floating-point
number~\cite{forsythe1959reprint}. RR is equivalent to applying Monte-Carlo
Arithmetic (MCA~\cite{parker1997monte}) to double-precision numbers with a
virtual precision of 53 bits and to single-precision numbers with a virtual
precision of 24 bits, which was shown to accurately simulate the effect of
operating system updates on the structural MRI pre-processing pipelines of the
Human Connectome Project (HCP) when applied to \emph{GNU
  libmath}~\cite{salari2021accurate}. Structural HCP pipelines consist of tools
assembled from the \emph{FSL}~\cite{jenkinson2012fsl} and
\emph{Freesurfer}~\cite{fischl2012freesurfer} toolboxes, which makes them
conceptually very similar to the structural \fmriprep pipeline targeted by our
study.

RR is rigorously implemented in several tools including
\emph{CADNA}~\cite{jezequel2008cadna}, \emph{Verrou}~\cite{fevotte2016verrou},
and \emph{Verificarlo}~\cite{denis2016verificarlo}. However, these tools incur
substantial performance overheads which makes them hard to apply to
compute-intensive applications. In addition, only \emph{Verrou} supports RR
instrumentation of \emph{GNU libmath} as a standalone
library~\cite{fevotte2019debugging}, and it does so by relying on quadruple
precision, which is not scalable to entire neuroimaging pipelines. Therefore,
we implemented a fast, approximate RR method by randomly adding or removing 1
ulp (unit in the last place) to the outputs of \emph{GNU libmath}'s functions.
Our implementation, available on
GitHub\footnote{\url{https://github.com/verificarlo/fuzzy/blob/master/docker/resources/libmath/fast/src/wrapping\_script.c}}
under Apache 2.0 license, only approximates RR as it applies a random
perturbation to an already rounded result instead of to the exact result as
done in rigorous implementations. In practice, computing the exact result
returned by \emph{GNU libmath}'s functions using tools like
\emph{MPFR}~\cite{fousse2007mpfr} is too costly for our use case.

Random Seed (RS) and RR trigger different types of variability. RR can be
applied transparently to any application while RS is more specific to the type
of analysis. Conversely, RR incurs a substantial performance overhead whereas
RS does not.

\subsection{Preprocessing of \fmriprep's outputs}
\label{subsec:preprocessing}

For the \fmriprep application, the main structural derivative produced is
$X_k$, that is, the T1-weighted MRI image corrected for intensity
non-uniformity using \texttt{N4BiasFieldCorrection} from \emph{ANTS} and
transformed to template space using \texttt{antsRegistration}. This file is
named \texttt{desc-preproc\_T1w} in the \fmriprep outputs. In addition to
$X_k$, \fmriprep produces a brain mask $B_k$, a segmentation into grey matter,
white matter and cerebrospinal fluid tissues, as well as probability maps for
each of these tissues.

Before computing the p-values in Equation~\ref{eq:pval}, we apply brain
masking, smoothing, and intensity normalization to $X_k$. For brain masking, we
mask $X_k$ with the union of the brain masks produced across all perturbed
results. We use the union of the brain masks rather than their intersection to
capture variability across $B_k$ masks.
% OE: unclear why smoothing seems a necessary step
For smoothing, we apply a spatial 3D Gaussian smoothing kernel with full-width
at half-maximum (\fwhm) ranging from 0\,mm to 20\,mm. The intensity values of
$X_k$ are normalized to the [0, 1] range with min-max scaling. The resulting
preprocessed image $X_k^\perp$ is used as input for the stability test.

\subsection{Numerical stability measure}
\label{subsec:sigbits}

As a by-product of test construction, we can measure numerical variability in
the application results, which provides valuable information about their
numerical quality. We quantify numerical stability for each voxel using the
number of significant bits as a metric. This metric helps distinguish the bits
containing signal from those containing only noise. We compute the number of
significant bits $\hat{s}$ with probability $p_s=0.95$ and confidence
$1-\alpha_s=0.95$ using the \emph{Significant Digits}
package\footnote{\url{https://github.com/verificarlo/significantdigits}}
(version 0.1.2). \emph{Significant Digits} implements the Centered Normality
Hypothesis approach described in~\cite{sohier2021confidence}:
\[
  \hat{s_i} = -\log_2 \left| \frac{\hat{\sigma_i}}{\hat{\mu_i}} \right| - \delta(n, \alpha_s, p_s),
\]

where $\hat{\sigma_i}$ and $\hat{\mu_i}$ are the voxelwise average and standard
deviation over the $X_k^\perp$ perturbed results ($k \leq n$), and
\begin{equation}
  \delta(n, \alpha_s, p_s) = \log_2 \left( \sqrt{\frac{n-1}{\chi^2_{1-\alpha_s/2}}} \Phi^{-1} \left( \frac{p_s+1}{2} \right) \right)
\end{equation}
is a penalty term for estimating $\hat{s_i}$ with probability $p_s$ and confidence level $1-\alpha_s$ for a sample size $n$.
$\Phi^{-1}$ is the inverse cumulative distribution of the standard normal distribution and $\chi^2$ is the Chi-2 distribution with $n$-1 degrees of freedom.

\subsection{Data}
\label{subsec:data}

We selected eight test subjects from sub-datasets in the
OpenNeuro~\cite{markiewicz2021openneuro} data-sharing platform, representing a
diversity of ages, sex, and study designs. The datasets include a motion study
with children (ds000256), a long-term memory study with young adults
(ds001748), and a motor process study with adults (ds002338). In addition, two
sub-datasets involve steps of the pipeline that can affect its reproducibility,
namely different field maps (ds001600) and non-structural images (ds001771).
Table~\ref{table:dataset_info} lists the dimension, voxels resolution, age and
sex of each subject in the dataset.

\begin{table*}
  \begin{center}
    \begin{tabular}{c|c|l|c|c|c|c|c}
      Index & Dataset  & Subject     & Dimension ($x,y,z$)         & Voxel resolution            & Data type & Age     & Sex \\
            &          &             &                             & $mm^3$ ($x,y,z$)            &           & (years) &     \\
      \hline
      1     & ds000256 & sub-CTS201  & $256 \times 256 \times 256$ & $1.0 \times 1.0 \times 1.0$ & int16     & 8.68    & M   \\
      2     & ds000256 & sub-CTS210  & $224 \times 256 \times 256$ & $0.8 \times 0.8 \times 0.8$ & int16     & 7.63    & F   \\
      3     & ds001600 & sub-1       & $176 \times 256 \times 256$ & $1.0 \times 1.0 \times 1.0$ & int16     & -       & -   \\
      4     & ds001748 & sub-adult15 & $176 \times 240 \times 256$ & $1.0 \times 1.0 \times 1.0$ & float32   & 21      & M   \\
      5     & ds001748 & sub-adult16 & $176 \times 240 \times 256$ & $1.0 \times 1.0 \times 1.0$ & float32   & 21      & F   \\
      6     & ds001771 & sub-36      & $256 \times 320 \times 320$ & $0.8 \times 0.8 \times 0.8$ & int16     & 22      & F   \\
      7     & ds002338 & sub-xp201   & $176 \times 512 \times 512$ & $1.0 \times 0.5 \times 0.5$ & int16     & 41      & F   \\
      8     & ds002338 & sub-xp207   & $176 \times 512 \times 512$ & $1.0 \times 0.5 \times 0.5$ & int16     & 39      & M   \\
    \end{tabular}
  \end{center}
  \caption{Dimension, voxels resolutions, age and sex of each subject in the dataset.}
  \label{table:dataset_info}
\end{table*}

\subsection{Computing infrastructure}
\label{subsec:computing_infrastructure}

% This sentence sounds like a necessary acknowledgment - move there?
We processed the dataset using the Narval cluster managed by Calcul Qu\'ebec
and part of the Digital Research Alliance of Canada. With our job submission
parameters, we could access 1,145 computing nodes with 64 cores per node and 2
$\times$ AMD Rome 7532 @ 2.40 GHz 256M cache L3. We executed \fmriprep in a
Singularity container built from a Docker image available on DockerHub
\texttt{yohanchatelain/fmriprep-fuzzy:20.2.1}.
% OE: suggestion - extract most relevant libraries and versions into its own table (and maybe add as a supplementary material)
The container image used \emph{Ubuntu} version 16.04.6 (LTS) with Linux kernel
4.18.0\footnote{Kernel build 4.18.0-372.19.1.el8\_6.x86\_64}, \emph{GNU
  libc/libmath} 2.23, and \fmriprep 20.2.1. We used \emph{Fuzzy}
\texttt{v0.9.1-a} built with \emph{Verificarlo} version \texttt{v0.9.1}.

Further spurious variability can source from the ever-changing initial point of
all random number generators reliant on stochastic processes that are involved
in the processing (e.g., optimization). To control for such variability,
\fmriprep's `master' random seed (\texttt{--random-seed=42}) as well as the
skull stripping random seed were fixed. We also fixed multi-threading to
perform calculations in a deterministic order.

\section{Results}

We computed $n$=30 perturbed results for each of the eight subjects and for the
two types of perturbations RR and RS. For RS, we used \fmriprep's command-line
interface to set the random seed in all the pipeline components. We also
computed an unperturbed IEEE result for each subject using the random seed used
in RR (42). For each type of perturbation, we measured the numerical stability
of \fmriprep in terms of significant bits, and we built the stability test
using different sizes of smoothing kernel and different confidence levels.

\subsection{Measured numerical variability was high and it varied across subjects}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/stats.pdf}
  \caption{Voxel-wise means of significant bits
    measured across n=30 perturbed samples for RR and RS perturbations and 8
    subjects.}
  \label{fig:significant-digits}
\end{figure}
Overall, the two types of perturbations (RR and RS) resulted in numerical uncertainties of comparable magnitude and behavior (Figure~\ref{fig:significant-digits}), which supports the validity of our results. The measured numerical variability was high, with mean significant bits ranging from 2.5 bits to 8.5 bits out of the 12 bits available in the data\footnote{The voxel intensity is encoded on 12 bits although it is embedded in a 16 bits format.}. The application appears to be highly sensitive to numerical and random seed perturbations.

We noted substantial discrepancies in numerical stability across subjects. For
a given smoothing kernel size, the number of significant bits frequently varied
in the ratio of 1 to 3 across subjects. Overall, smoothing tended to reduce
numerical variability, however, this behavior was in general not monotonous and
impacted subjects differently. The observed between-subject variability
demonstrates the importance of evaluating such applications on a representative
set of datasets.

The numerical variability measured across perturbed samples showed regional
variations compatible with anatomical features
(Figure~\ref{fig:uncertainty-maps} and
Appendix~\ref{appendix:numerical_uncertainty}). In particular, variability was
maximal at the border of the brain mask, and it was overall higher in the gray
matter than in the white matter. This is consistent with previous observations
of numerical variability in structural brain image
analysis~\cite{salari2021accurate}. In addition, numerical variability was also
maximal in some focal regions, suggesting that spatial normalization may be
unstable in these regions. Our stability test will be more permissive,
resulting in fewer rejections of voxels in these regions.

\begin{figure*}
  \uncertaintyMapDiscreteAll{15}
  \vspace*{-20pt}\hspace{15pt}
  \includegraphics*[width=.6\linewidth]{figures/colorbar_sigbit_discrete.pdf} \\
  \vspace*{-5pt}
  significant bits
  \caption{Numerical variability measured for subjects 1 to 8 (from top to bottom) across n=30 perturbed samples, with FWHM\,=\,15\,mm. Uncertainty maps for other smoothing kernel sizes are available in Appendix~\ref{appendix:numerical_uncertainty}.}
  \label{fig:uncertainty-maps}
\end{figure*}

\subsection{The results stability test passed sanity checks but usable FWHM and \texorpdfstring{$\alpha$}{α} values were data-dependent}

We implemented three different sanity checks to evaluate the relevance of the
stability test. The leave-one-out check (\ref{subsec:loo_check}) evaluates the
specificity of the stability test, that is, its ability to accept results
produced by the reference application $\Lambda$. The IEEE check
(\ref{subsec:ieee_check}) evaluates the specificity of the test by checking
that it accepts the unperturbed application result and its sensitivity through
the rejection of results produced by a different subject than the original
result. The corrupted template check (\ref{subsec:template_check}) evaluates
the sensitivity of the stability test, that is, its ability to reject results
produced by a corrupted version of the reference application.

\subsection{Leave-one-out check}
\label{subsec:loo_check}
We implemented a ``leave-one-out" (LOO) evaluation by constructing the stability test $n$ times for $n-1$ perturbed results and applying it to the remaining perturbed result. By construction, the remaining perturbed result is sampled from the distribution of results produced by $\Lambda$ and should therefore be accepted by the stability test.
To define a clear passing criterion for the LOO check, we modeled the LOO check using a binomial variable $B(n,1-\alpha)$ where $n$ is the number of LOO iterations and $1-\alpha$ is the probability that a perturbed sample is accepted by the stability test. Under $H_0$ for all voxels, we expect the following bound to be verified:
\[
  1-F(\mathds{1}_n;n,1-\alpha) \leq \alpha_0
\]
where $F(x;n,p)$ is the cumulative distribution function of the Binomial law
$B(n,p)$, and $\alpha_0=0.05$.

\paragraph*{Results} We applied the LOO check for different confidence values ($1-\alpha$) and different FWHM  values for the RR and RS perturbations (Figure~\ref{fig:loo_bonferroni}). As expected, the stability test became increasingly permissive for increasing values of $\alpha$ (reduced confidence) and increasing values of FWHM. As previously observed, RR and RS behaved similarly overall. For each subject, there were values of $\alpha$ and FWHM such that the LOO check passed, which demonstrates the good specificity of the stability test.

However, the values of $\alpha$ and FWHM for which the LOO check passed
importantly varied across subjects, presumably due to between-subject
variations in input data quality and instability modes of spatial
normalization. For instance, to pass the LOO check with $\alpha=0.05$ for RR
perturbations, subjects 5, 6, 7 and 8 required a smoothing size of
FWHM\,=\,12\,mm and subjects 1 and 4 required FWHM\,=\,15\,mm. Subjects 2 and 3
required $\alpha=0.15$ to pass the LOO check. Such discrepancies are not
surprising given the heterogeneity in numerical variability previously observed
across subjects. In practice, different values of $\alpha$ and FWHM must be
used for each subject in the test dataset.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/loo_fwe_bonferroni.pdf}
  \caption{Leave-one-out evaluation of stability test for subjects 1 to 8.
    Red/green: rejected/accepted by a binomial one-tailed test with sample size $n=30$ and confidence level $1-\alpha_0=0.95$.}
  \label{fig:loo_bonferroni}
\end{figure}

\subsection{IEEE check}
\label{subsec:ieee_check}

We constructed the stability test from the $n$ perturbed results for each
subject and applied it to the IEEE results (one per subject). The purpose of
this check was twofold: (within subjects) to verify that IEEE results passed
the stability test built from the reference distribution of their corresponding
subject and (between subjects) to verify that IEEE results failed the stability
test built from the reference distribution of other subjects.

\paragraph*{Within subjects} for each subject, there was an $\alpha$ and FWHM pair such that all the within-subject IEEE checks passed (Figure~\ref{fig:ieee-check}). In particular, the within-subject IEEE check passed with FWHM\,=\,15\,mm for all subjects and $\alpha$ values for RR perturbations. For low FWHM sizes, the stability test rejected the IEEE sample of the reference subject, suggesting a lack of specificity in such cases.

\paragraph*{Between subjects} The stability tests successfully rejected all the IEEE samples coming from other subjects, for all combinations of $\alpha$ and FWHM values, and consistently for both RR and RS perturbations. Therefore the stability test is sufficiently sensitive to detect between-subject variability even with a high smoothing kernel size.

We conclude from this experiment that the stability test is sensitive enough to
reject results obtained from different subjects using the reference application
and accept results obtained from the same subject using the reference
application executed with random perturbations.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/inter-subject/inter_mct_fwe_bonferroni.pdf}
  \caption{Ratios of successful stability tests for the within-subject IEEE check. The between-subject IEEE check (not displayed in the figure) successfully rejected data computed from different subjects for all $\alpha$ and FWHM combinations, demonstrating excellent sensitivity.
  }
  \label{fig:ieee-check}
\end{figure}

\subsection{Corrupted template check}
\label{subsec:template_check}

Multiple brain templates exist to spatially normalize subject data to a common
space, and template selection substantially impacts the
results~\cite{li2021moving}. Hence, errors in the template should lead to
substantial differences in the results. The purpose of this check was to verify
that results obtained from corrupted templates were correctly rejected by the
stability test. To do so, we generated corrupted versions of the
MNI152NLin2009cAsym template used by \fmriprep where we incrementally zeroed
the intensity of an increasing fraction of brain voxels selected uniformly. We
then executed \fmriprep in IEEE mode (without random perturbations) for each
resulting corrupted template and each subject.

\paragraph*{Results} The stability test correctly rejected results obtained with the corrupted template above a subject-dependent threshold of corrupted voxels (Figure~\ref{fig:template_bonferroni}). Smoothing had a measurable effect that did not manifest clearly across subjects. We conclude that the stability test is sensitive to changes in the brain template which is a common source of variability in neuroimaging.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/template/template_fwe_bonferroni.pdf}
  \caption{Corrupted template check for RR (top) and RS (bottom) modes. The stability tests correctly rejected results produced with corrupted templates beyond a subject-dependent threshold of corrupted voxels.}
  \label{fig:template_bonferroni}
\end{figure}

\subsection{The results stability test detected subtle updates in \fmriprep image processing methods}

The previous sanity checks demonstrated a satisfying level of sensitivity and
specificity of our stability test for simple scenarios. Subsequently, we
applied it to our main motivating use case: the detection of results
differences in \fmriprep LTS versions. We tested the results produced by
different versions of \fmriprep, having built the results stability test for
version 20.2.1. The goal of this experiment was to evaluate the ability of the
stability test to detect significant software updates in \fmriprep. The
stability test accepted results produced by versions 20.2.0 to 20.2.4 for all
subjects (Figure~\ref{fig:version_bonferroni}). However, the test rejected
results produced by versions 20.2.5 and onwards, suggesting a substantial
change in the analysis methods from version 20.2.5. Investigations with the
\fmriprep developers revealed that the interpolation scheme involved in the
resampling of piece-wise-smooth brain segmentation maps from
\emph{FreeSurfer}'s preferred format (MGZ) format into NIfTI 1.0 format was
changed in version {20.2.5} from trilinear interpolation to nearest neighbor
(see pull-request
\href{https://github.com/nipreps/smriprep/pull/268}{\url{github.com/nipreps/smriprep/pull/268}}).
The ability of our results stability test to detect such subtle changes in the
analysis supports its relevance in the release process of long-term support
data analysis software.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/fmriprep-versions/versions_fwe_bonferroni.pdf}
  \caption{Application of the stability test across \fmriprep versions, with the reference distribution constructed using version 20.2.1. The test indicated a divergence in the results beginning with version 20.2.5. Investigations showed a substantial change in the \fmriprep analysis methods since version 20.2.5, correctly detected by the stability test.}
  \label{fig:version_bonferroni}
\end{figure}

\section{Discussion}

% summary of results

We introduced a novel approach to build results stability tests for data
analyses using numerical variability, with a specific focus on neuroimaging and
the widely used \fmriprep software. While libraries to test numerical stability
exist and have been applied in other contexts, to our knowledge, this is one of
the first attempts to systematically apply such an approach in the domain of
neuroimaging and possibly in other data analysis domains. This approach does
not require an exact solution, a reference solution, or even an acceptable
bound of variation around the computed solution. Applying this approach to
\fmriprep, a highly popular neuroimaging preprocessing pipeline, we
demonstrated non-negligible, subject-specific numerical errors that the tool
introduces in the results as a consequence of numerical instability. In
addition, we showed how the framework successfully detected subtle algorithmic
updates through \fmriprep releases, while remaining specific enough to accept
numerical variations within a reference version of the application. Based on
our findings, we contend that benchmarking neuroimaging and other computational
tools is essential. This not only includes accuracy against \emph{golden}
standards in cases where a ground-truth reference is absent and performance but
also numerical stability.

Our stability test represents a practical tool that has the potential for
widespread application, not just limited to the \fmriprep development
environment. Our results stability test is compatible with continuous
integration platforms, enabling automated validation of new commits. By
choosing suitable data and confidence values, our results stability test can
serve as a valuable addition to conventional code testing methodologies.
Additionally, the figure templates presented in this paper can be reused to
create informative visualizations for dashboards or similar data presentation
platforms. This way, the results are easily interpretable and actionable.

% \subsection*{Statistical test}
Designing the results stability test involved multiple choices that we discuss
hereafter. First, we employed a parametric statistical test
(Equation~\ref{eq:pval}) that is less compute-intensive than non-parametric
methods \add{such as the bootstrap approach~\cite{zrimvsek2024quantifying}}
that typically demand larger sample sizes --- in our case, more than $100$
repetitions per subject --- to achieve acceptable confidence levels ($\geq
  0.95$). Indeed, the computational and storage costs required to apply
stochastic arithmetic to neuroimaging tools such as \fmriprep are high,
although they are incurred only during the construction of the test and not
during its evaluation, which therefore does not impact test users. \add{We
  chose the z-test for 1) the sample size ($n = 30$), allowing to rely on the law
  of large numbers, and 2) the need to control for multiple comparisons
  ($\simeq10^7$ voxels), where managing multiple comparisons outweighs concerns
  about the exact distribution.} Moreover, our stability test operates at the
level of individual voxels and is accepted or rejected globally for a given
image. Refining the test by regions of interest in the image could provide
additional insights about the application behavior. The Bonferroni correction
for multiple comparisons is conservative in our case given that the voxel
intensities in a brain image are not independent. Possible alternatives to this
correction are presented in~\cite{NICHOLS2007246}. The use of Bonferroni
correction reduces the sensitivity of our test and increases its specificity.
\add{Regarding the alpha value, we recognize that the 0.05 threshold is often
  used without clear justification. To mitigate this binary decision, a Bayesian
  approach could offer a continuous probability measure of similarity between
  images, allowing for a more nuanced interpretation of the results.}

% \subsection*{Perturbation modes}
The random rounding method adopted to sample the null distribution of results
produced by the reference application can be applied regardless of the type of
input and output data involved. \add{Our assumption is that under normal
  conditions—without significant code changes—numerical variability remains
  within established bounds. Any significant deviation would signal instability
  introduced by external factors or updates.} In our \fmriprep use case, random
rounding resulted in comparable variability to the one obtained from varying
the random seed, which increased the confidence in our results. However, it is
important to note that random rounding and random seed are different types of
perturbations, and there is no guarantee of producing comparable variability
patterns in general. We applied random rounding to functions of the \emph{GNU
  libmath} library since previous experiments had shown that neuroimaging
analyses commonly rely on these functions. Other types of applications may
require that random rounding be applied to other libraries. For instance, the
work in~\cite{gonzalez2024numerical} reports on the use of random rounding for
deep neural networks, which required instrumenting the entire Tensorflow
application and its dependencies.

%\subsection*{Pre-processing (smoothing)}
The size of the spatial smoothing kernel had a notable impact on the
performance of the stability test. Smoothing was required for the test to
accept results produced by the reference application and therefore reach an
acceptable level of specificity. The smoothing process transforms the
distribution of voxel intensities towards a Gaussian distribution, thus better
aligning with the main assumption of the test. Smoothing is a prevalent
operation in neuroimage analysis to improve the signal-to-noise ratio. However,
FWHM values usually remain lower than 8\,mm while our test required 15\,mm for
most subjects. It would be interesting to further investigate why such large
smoothing kernel sizes were required since large smoothing kernel sizes reduce
the sensitivity of our test.

%\subsection*{Dataset}
The results stability test behaved quite differently across the 8 test
subjects, which motivates the inclusion of various datasets in such test cases.
To build our test, we selected subjects to reflect diverse acquisition
parameters, resolutions, ages, and sexes. However, it would be interesting to
investigate two distinct effects: (1) differences between subjects, and (2)
differences due to scanners or sequence parameters. For the first case, using a
dataset from a single study is more appropriate since it consists of subjects
who have undergone identical experimental protocols and procedures, thereby
ensuring better control over acquisition parameters. In the second case, to
examine the impact of differing scanners or sequence parameters, a traveling
subject approach would be more effective. In this method, a single subject
undergoes MRI scans at various locations, using different scanners or imaging
protocols. This controlled approach provides valuable insight into the
influence of these variables.

%\subsection*{Future work}
In future work, we plan to extend our methodology to other data types and
investigate its applicability under different statistical hypotheses. Results
stability tests could be applied to diverse scientific domains beyond
neuroimaging, further enhancing the reliability and reproducibility of
computational results. In the short term, we plan to extend our test to
functional neuroimaging data, which involves 4D images. \add{Notably, it has
  been shown that different preprocessing tools can yield varying results,
  potentially impacting conclusions~\cite{olszowy2019accurate}.}

\section*{Acknowledgments}

Computations were made on the Narval supercomputer from \'Ecole de Technologie
Sup\'erieure (ETS, Montr\'eal), managed by Calcul Québec and the Digital
Research Alliance of Canada. The operation of this supercomputer is funded by
the Canada Foundation for Innovation (CFI), Ministère de l’Économie, des
Sciences et de l’Innovation du Québec (MESI) and le Fonds de recherche du
Québec – Nature et technologies (FRQ-NT).

\bibliographystyle{IEEEtran}
\bibliography{main}

\section{Biography Section}

\vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
%   Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photo/yohan_chatelain.jpeg}}]{Yohan Chatelain}
  is a postdoctoral researcher in the
  Big Data Infrastructures for Neuroinformatics lab at the University of
  Concordia, Montreal, Canada.
  He received his Ph.D. degree from the University of Paris-Saclay
  (UVSQ), Versailles, France in 2019. Yohan research topics include computer
  arithmetic, high-performance computing, and reproducibility. Yohan aims at
  democratizing the use of the analysis of stability for scientific computing
  codes through automatic tools to improve numerical quality.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photo/loic_tetrel.jpeg}}]{Loïc Tetrel}
  is an R\&D Engineer on Kitware Europe’s computer vision team.
  Prior to joining Kitware, Loïc was a Data Scientist in Neuroscience at SIMEXP lab (University of Montreal) where he worked on neuroimaging software, machine learning, and research data platforms. He contributed to TensorFlow, Nilearn and Binderhub. Loïc also worked at Straumann Group as a computer vision engineer for 2 years, mainly working on 3D reconstruction algorithms.
  He holds an engineering degree from INSA Lyon and an M.A.Sc from ETS Montréal.
  At Kitware Europe, Loïc brings his competencies in Machine Learning and 3D modeling to the team, in particular in the medical domain.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photo/christopher_markiewicz.jpeg}}]{Christopher J. Markiewicz}
  is a software developer for the Poldrack Lab at Stanford University and a research affiliate of the McGovern Institute for Brain Research. He has been a core developer of fMRIPrep and FitLins, and is a contributor to and maintainer of several open-source, Python neuroimaging libraries, including Nipype and NiBabel.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photo/mathias_goncalves.jpeg}}]{Mathias Goncalves}
  is a software developer in the Department of Psychology at Stanford University, working in the Poldrack Lab. He is interested in building and improving tools for neuroimaging analysis and develops several open-source projects including fMRIprep and Nipype.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photo/gregory_kiar.jpeg}}]{Gregory Kiar} is a research scientist at the Child Mind Institute. Throughout his
  degrees in Biomedical Engineering, Greg has developed techniques to study
  biosignal data, a turn-key tool that allows researchers to generate maps of
  brain connectivity from diffusion MRI data, and techniques to assess and
  improve the stability of neuroscience research. Greg’s research bridges
  the fields of numerical analysis and brain imaging to evaluate and improve
  the trustworthiness of techniques used to study the brain.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photo/oscar_esteban.jpeg}}]{Oscar Esteban}
  is a Research and Teaching Ambizione FNS Fellow at the Department of Radiology of the Lausanne University Hospital (CHUV) and the University of Lausanne, and leads the \href{https://www.axonlab.org}{Axon Lab} research group.
  The Axon Lab focuses on applying fundamental signal-processing principles to robustly analyze and interpret multimodal neural data, thereby pushing `network neuroscience' forward.
  Within the magnetic resonance imaging (MRI) field, Dr. Esteban is known to have been at the core of \textit{fMRIPrep},
  a popular functional MRI (fMRI) preprocessing pipeline.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photo/pierre_bellec.jpeg}}]{Pierre Bellec}
  is the principal investigator of the laboratory for brain simulation and exploration (SIMEXP), as well as the scientific director of the Courtois Project on Neuronal Modelling (CNeuroMod), which uses human neuroimaging data to help train large artificial neural networks on a variety of cognitive tasks. He is also the director of Unité de neuroimagerie fonctionnelle at the “Centre de recherche de l’institut de gériatrie de Montréal” and an associate professor at the Psychology department at Université de Montréal. Dr Bellec is a senior fellow (”chercheur boursier senior”) of the ”Fonds de Recherche du Québec - Santé”.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photo/tristan_glatard.jpeg}}]{Tristan Glatard} is Associate Professor in the Department of Computer Science and
  Software Engineering at Concordia University in Montreal, Canada
  Research Chair (Tier II) on Big Data Infrastructures for Neuroinformatics.
  Before that, he was a research scientist at the French National Centre for
  Scientific Research and Visiting Scholar at McGill University.
\end{IEEEbiography}

\vfill

\appendix

\section*{Numerical variability results for different smoothing kernel sizes}
\label{appendix:numerical_uncertainty}

\begin{figure*}[h]
  \centering
  \uncertaintyMapDiscreteAll{0}
  \vspace*{-20pt}\hspace{15pt}
  \includegraphics*[width=.7\linewidth]{figures/colorbar_sigbit_discrete.pdf} \\
  \vspace*{-5pt}
  significant bits
  \caption{Numerical variability measured for subjects 1 to 8 (from top to bottom) across n=30 perturbed samples, with FWHM\,=\,0\,mm. }
  \label{fig:uncertainty-maps-0mm-disc}
\end{figure*}

\begin{figure*}
  \centering
  \uncertaintyMapDiscreteAll{5}
  \vspace*{-20pt}\hspace{15pt}
  \includegraphics*[width=.7\linewidth]{figures/colorbar_sigbit_discrete.pdf} \\
  \vspace*{-5pt}
  significant bits
  \caption{Numerical variability measured for subjects 1 to 8 (from top to bottom) across n=30 perturbed samples, with FWHM\,=\,5\,mm. }
  \label{fig:uncertainty-maps-5mm-disc}
\end{figure*}

\begin{figure*}
  \vspace*{-2cm}
  \centering
  \uncertaintyMapDiscreteAll{10}
  \vspace*{-20pt}\hspace{15pt}
  \includegraphics*[width=.7\linewidth]{figures/colorbar_sigbit_discrete.pdf} \\
  \vspace*{-5pt}
  significant bits
  \caption{Numerical variability measured for subjects 1 to 8 (from top to bottom) across n=30 perturbed samples, with FWHM\,=\,10\,mm. }
  \label{fig:uncertainty-maps-10mm-disc}
\end{figure*}

\begin{figure*}
  \vspace*{-2cm}
  \centering
  \uncertaintyMapDiscreteAll{20}
  \vspace*{-20pt}\hspace{15pt}
  \includegraphics*[width=.7\linewidth]{figures/colorbar_sigbit_discrete.pdf} \\
  \vspace*{-5pt}
  significant bits
  \caption{Numerical variability measured for subjects 1 to 8 (from top to bottom) across n=30 perturbed samples, with FWHM\,=\,20\,mm. }
  \label{fig:uncertainty-maps-20mm-disc}
\end{figure*}

% \section{Colors Palette}
% Fail: \texttt{\#D55E00}
% Pass: \texttt{\#009E73}

% \section{Results for uncorrected tests}
% \label{appendix:multiple-comparison-tests}

% In the absence of multiple comparison corrections, we expect
% by construction to incorrectly reject each $H_{0,i}$ with probability $\alpha$ a.k.a
% the per-comparison error rate (PCER). To account for the PCER, we measure the
% fraction of positive z-tests $V_P$ among the $v$ voxels in the image and the
% tested \fmriprep result is considered part of the reference distribution iif:
% \begin{equation}
%     V_{P} \leq \alpha.
%     \label{eqn:pce}
% \end{equation}

% For each of the $n$ repetitions,
% we measured $V_P$, the fraction of
% positive z-tests among the $v$ voxels as well as $\mathds{1}_n$, the number of repetitions
% that passed the test with Bonferroni correction.

% For the uncorrected tests (Equation~\ref{eqn:pce}), we expect $\overline{V_P}$ to have
% the following upper bound:
% \[
%     \overline{V_P} \leq
%     \alpha  + t_{29,0.05} \frac{\tilde{\sigma}_{V_P}}{\sqrt{30}}
% \]
% with
% $t_{k,\gamma}$ the $(1-\gamma)$quantile of the Student distribution with $k$ degrees of freedom,
% and $\overline{V_P}$ and $\tilde{\sigma}_{V_P}$ the mean and standard-deviation estimated from
% $n$ $V_P$ measures.
% This confidence interval is obtained from a two-tailed one-sample
% t-test with $n-1$ degrees of freedom at a level of significance $\alpha_0=0.05$:
% \[
%     \mathbb{P}
%     \left(
%     -t_{n-1,\alpha_0/2}
%     <
%     \dfrac{\overline{V_p} - \alpha}{\tilde{\sigma}_{V_P} / \sqrt{n}}
%     <
%     t_{n-1,\alpha_0/2}
%     \right)
%     = 1 - \alpha_0
% \]

% fraction of positive
% z-tests $V_P$ among the $v$ voxels should be close to the nomimal value $\alpha$
% on average. To assess the validity of our LOO test, we test that the average
% of the $n$ $V_P$ fractions measured belongs to a confidence interval
% around the nomimal value $\alpha$. To do so, we use  resulting in the
% following confidence interval for $V_P$:

% obtained from:

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/inter-subject/inter_pce.pdf}
%     \caption{Inter-subject check for the uncorrected test for RR (top) and RS (bottom) modes. We expect that the tests would pass along the diagonals and be rejected otherwise.
%         Out of a total of 17,472 individual assessments, we reported 791 failures in the RR mode, representing a failure rate of 0.05\%, and 685 failures in the RS mode, corresponding to a failure rate of 0.04\%. These data corroborate our test's capacity to discern inter-subject variability.
%     }
%     \label{fig:ieee-check-pce}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/fmriprep-versions/versions_pce.pdf}
%     \caption{Uncorrected test applications on different \fmriprep versions.}
%     \label{fig:versions_pce}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/arch/arch_pce.pdf}
%     \caption{Uncorrected test applications on CPU architectures.}
%     \label{fig:arch_pce}
% \end{figure}

% \section{Uncertainty maps for others FWHM}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/loo_pce.pdf}
%     \caption{PCE}
%     \label{fig:loo_pce}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/template/template_pce.pdf}
%     \caption{Corrupted template check for RR and RS modes (pce)}
% \end{figure}

\end{document}

